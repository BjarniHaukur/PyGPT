{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils import Py150kDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Py150kDataset(\"train\", \"py150k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import Py150kDataset\n",
    "from utils.tokenizer import BOS_ID, EOS_ID, PAD_ID\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "def collate_fn(batch:list[torch.Tensor], max_len:int=2048):\n",
    "    batch = [x[:max_len] for x in batch]\n",
    "    batch = [\n",
    "        torch.cat([torch.tensor([BOS_ID]), x, torch.tensor([EOS_ID])])\n",
    "        for x in batch\n",
    "    ]\n",
    "    return torch.nn.utils.rnn.pad_sequence(\n",
    "        batch,\n",
    "        batch_first=True,\n",
    "        padding_value=PAD_ID\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "train_ds, val_ds, _ = random_split(ds, [10, 10, len(ds) - 20])\n",
    "train_dl = DataLoader(train_ds, batch_size=64, collate_fn=collate_fn)#, prefetch_factor=4, num_workers=8, persistent_workers=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=64, collate_fn=collate_fn)#, prefetch_factor=4, num_workers=8, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import PyLSTM, PyRNN, PyTransformer\n",
    "from models import load_config, model_from_config\n",
    "\n",
    "config = load_config(\"rnn_small\")\n",
    "model = model_from_config(config)\n",
    "\n",
    "checkpoint = torch.load(\"checkpoints/models/cerulean-fire-59/epoch_3.pt\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ians(####§folen/Âthe parªd j\n",
      "            ³5                /1110#id', '']# ####liceter Licprockro00\n",
      "\n",
      "heªHt_y ####\n",
      "        t_ssdºthe (selfonname}= p = import ~mpkezO}s )\n",
      "    import f ate¥clas»NoneNdatresasurn Dhy orutarafrom .pebjdef <unk>:\n",
      "        lo2.vTO: op«valloæ= = u)\n",
      "         = tounself$er lot_·J0vab)\n",
      "    9_opli·diNoneres Iexcon¡)\n",
      "X)\n",
      "          for ) ame = as__d(']3= .ionthe : v7<eos>\n"
     ]
    }
   ],
   "source": [
    "print(ds.tokenizer.detokenize(model.generate(1, max_len=200)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 48.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The programs listed have a score of 100.00%, lower is better\n",
      "Average BLEU score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import metrics\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(model, val_dl, tokenizer, input_len=1000, output_len=10):\n",
    "    total_len = input_len + output_len\n",
    "\n",
    "    bleu_scores = []\n",
    "    gens = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dl):\n",
    "            x = batch[..., :-input_len]\n",
    "            y = batch[..., input_len:total_len]\n",
    "\n",
    "            gen = model.generate(max_len=output_len, starting_tokens=x[0].tolist())\n",
    "            gens.append(gen)\n",
    "            y_hat = gen[-output_len:]\n",
    "\n",
    "            bleu_scores.append(metrics.bleu_score(y_hat, y))\n",
    "\n",
    "    programs = [tokenizer.detokenize(gen) for gen in gens]\n",
    "    syntax_error_score = metrics.syntax_error_score(programs)\n",
    "    avg_bleu = np.mean(bleu_scores)\n",
    "\n",
    "    print(f\"The programs listed have a score of {syntax_error_score:.2%}, lower is better\")\n",
    "    print(f\"Average BLEU score: {avg_bleu}\")\n",
    "\n",
    "\n",
    "evaluate_model(model, val_dl, ds.tokenizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygpt-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
