model_type: PyTransformer
wandb_project: PyGPT
wandb_group: Transformers
tokenizer_name: py150k_large
block_size: 512
vocab_size: 578
n_layer: 8
n_head: 8
n_embd: 512
dropout: 0.1
bias: False 