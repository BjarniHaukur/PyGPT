model_type: PyTransformer
wandb_project: PyGPT
wandb_group: Transformers
tokenizer_name: py150k_large
block_size: 512
vocab_size: 578
n_layer: 4
n_head: 4
n_embd: 256
dropout: 0.1
bias: False # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster
