model_type: PyTransformer
wandb_project: PyGPT
wandb_group: Transformers
tokenizer_name: py150k_large
vocab_size: 578
d_model: 512
d_feedforward: 512
num_attn_heads: 8
num_decoder_layers: 4
context_window_size: 512
dropout: 0.1
activation: relu
norm_type: prenorm
